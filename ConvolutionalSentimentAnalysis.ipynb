{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5pfTmoumLAp"
   },
   "source": [
    "\n",
    "# Deuxième approche de changement des appréciations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN6Iue50NWGo"
   },
   "source": [
    "## 1 - Installation des packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ghKU0TFNTJXn",
    "outputId": "3fc047e3-c8b4-4c13-c0ee-e6f96baddc03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.7.0+cu101)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.8.1+cu101)\n",
      "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (0.3.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch) (0.8)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.41.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.23.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zk1BknpyTwVk",
    "outputId": "92a59422-7c00-43b9-dac4-eaad4ad12ce4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (51.3.3)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
      "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.3.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
      "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "! python3 -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8KLfNpGzxtOE",
    "outputId": "67c5a75b-c9ea-46ff-9eda-a2f032994d7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting msgpack==0.5.6\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/4e/dcf124fd97e5f5611123d6ad9f40ffd6eb979d1efdc1049e28a795672fcd/msgpack-0.5.6-cp36-cp36m-manylinux1_x86_64.whl (315kB)\n",
      "\u001b[K     |████████████████████████████████| 317kB 7.5MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: msgpack\n",
      "  Found existing installation: msgpack 1.0.2\n",
      "    Uninstalling msgpack-1.0.2:\n",
      "      Successfully uninstalled msgpack-1.0.2\n",
      "Successfully installed msgpack-0.5.6\n"
     ]
    }
   ],
   "source": [
    "!pip install msgpack==0.5.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJ1EbdHcLuWR"
   },
   "source": [
    "## 2 - CNN pour traitment de texte\n",
    "\n",
    "On va utiliser un réseau de neuronne de convolution pour faire l'analyse de sentiment des différents mots qui composent les appréciations.\n",
    "\n",
    "Les réseaux de neuronnes à convolutions sont généralement utilisés pour le traitement des images and qu'on av les utiliser pour traiter du texte dans ce projet. \n",
    "\n",
    "L'idée est que dans une image 2 pixel qui sont cote à cote sont relié de la même façon que deux mots cote à cote sont relié. En même temps un CNN cherche à trouver des patters dans les images, dans ce cas de notre projet, il va essayer de chercher n-grams (en utilisant des filtre 1xn)\n",
    "\n",
    "L'idée principale dérrière àa est que l'apparence de certaines 2 grams, 3 grams ... vont jouer un rôle pour définir le sentiment final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xel7fyT3LuWS"
   },
   "source": [
    "## 3 - Preparation de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6p4e0_M3LuWT"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import random\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "TEXT = data.Field(tokenize='spacy')\n",
    "LABEL = data.LabelField()\n",
    "\n",
    "train, test = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "train, valid = train.split(random_state=random.seed(SEED))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "miIxzbHaXLEy",
    "outputId": "91909858-5037-4720-86d6-6b65c89a68be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OfKsp49rLuWY"
   },
   "source": [
    "## 4 - Importer le modèle word embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gVoAfhRPLuWZ",
    "outputId": "dbd0b274-210c-47db-d8fc-67d7c4c0e6b1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [06:27, 2.23MB/s]                          \n",
      "100%|█████████▉| 398045/400000 [00:15<00:00, 26373.16it/s]"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(train, max_size=25000, vectors=\"glove.6B.100d\")\n",
    "LABEL.build_vocab(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w518rTPFw7f4",
    "outputId": "8a9f85e5-6bad-490c-d6aa-9b9fea0ed555"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degrees\n",
      "dental\n"
     ]
    }
   ],
   "source": [
    "test_w=TEXT.vocab.itos[9205]\n",
    "test_w2=TEXT.vocab.itos[9206]\n",
    "\n",
    "print(test_w)\n",
    "print(test_w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MD4vi9in74Qo",
    "outputId": "1ef25377-103d-4523-eba9-34a2deee8e3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(values=tensor([-0.3674, -0.3650, -0.3144,  ...,  0.6289,  0.7273,  1.0000]), indices=tensor([21527, 18999, 14249,  ..., 13863,  2524,  9205]))"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.functional import cosine_similarity\n",
    "test_v=TEXT.vocab.vectors[9205].unsqueeze(0)\n",
    "test_v2=TEXT.vocab.vectors[9206]\n",
    "\n",
    "cosine_similarity(test_v,TEXT.vocab.vectors,dim=1).sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "id": "pDkI8M1dvOrH",
    "outputId": "e51c4e89-c026-4d3b-863e-16b5fe2bda8f"
   },
   "outputs": [],
   "source": [
    "print(TEXT.vocab.itos[538])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xzlpJr3eLuWc"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train, valid, test), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sort_key=lambda x: len(x.text), \n",
    "    repeat=False,\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPFkflGKLuWg"
   },
   "source": [
    "\n",
    "\n",
    "```\n",
    "# Ce texte est au format code\n",
    "```\n",
    "\n",
    "## 5 - Le Modele\n",
    "\n",
    "Les images ont deux dimensions alors que les texte ont juste une dimension, mais si on convertie un mot en des vecteurs avec les techniques de NLP (comme word embedding ) on peut avoir une représentation en deux dimensions.\n",
    "\n",
    "\n",
    "On considère ci dessous la représentation d'une phrase en deux dimensions en utilisant le word embedding. nots mots sont représentés en vert. On a 4 mots et 5 dimensions d'embedding ce qui créer une image de **[4x5]**.\n",
    "\n",
    "![](https://i.imgur.com/ci1h9hv.png)\n",
    "\n",
    "Puis un filtre qui couvre deux mots est déssiné en jaune. La sortie de ce filtre est un nombre réel.\n",
    "\n",
    "On peut donc considérer un filtre **[n x emb_dim]**. Ça va donc couvrir n mots séquentielles.\n",
    "\n",
    "![](https://i.imgur.com/QlXduXu.png)\n",
    "\n",
    "Le filtre doit passer sur toute l'image.\n",
    "\n",
    "![](https://i.imgur.com/wuA330x.png)\n",
    "\n",
    "![](https://i.imgur.com/gi1GaEz.png)\n",
    "\n",
    "Dans notre modèle, on va aussi utiliser des filtres avec différents tailles 3, 4, 5, 100 ... afin de poivoir prendre en compte les (différents n-grams : 3 grams, 2 gram ... ) pour savoir le sentiment de la phrase\n",
    "\n",
    "La prochaine étape du modèle est d'utiliser des couches de pooling après des couches de convolution. Ci dessous un exemple, du fait q'uon prend la valeur maximum 0.9 de la dernière couche de convolution\n",
    "\n",
    "![](https://i.imgur.com/gzkS3ze.png)\n",
    "\n",
    "L'idée ici, est que la valeur maximal est celle qui détérmine le plus important n-gram.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VtfYscMdLuWh"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.conv_0 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[0],embedding_dim))\n",
    "        self.conv_1 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[1],embedding_dim))\n",
    "        self.conv_2 = nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(filter_sizes[2],embedding_dim))\n",
    "        self.fc = nn.Linear(len(filter_sizes)*n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):          \n",
    "        x = x.permute(1, 0)\n",
    "                       \n",
    "        embedded = self.embedding(x)                \n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "             \n",
    "        conved_0 = F.relu(self.conv_0(embedded).squeeze(3))\n",
    "        conved_1 = F.relu(self.conv_1(embedded).squeeze(3))\n",
    "        conved_2 = F.relu(self.conv_2(embedded).squeeze(3))         \n",
    "        \n",
    "        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n",
    "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
    "        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n",
    "                \n",
    "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim=1))\n",
    "\n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF-CF9NOLuWj"
   },
   "source": [
    "Pour l'instant notre modèle CNN utilise que 3 différents filtres. mais c'est possible d'améliorer le code en utilisant la fonction nn.ModuleList qui prend en paramètresnn.Module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6n31qBsFLuWk"
   },
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(in_channels=1, out_channels=n_filters, kernel_size=(fs,embedding_dim)) for fs in filter_sizes])\n",
    "        self.fc = nn.Linear(len(filter_sizes)*n_filters, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):        \n",
    "        x = x.permute(1, 0)\n",
    "                        \n",
    "        embedded = self.embedding(x)\n",
    "                        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "                \n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "                    \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "              \n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "            \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJqPRQpJLuWp"
   },
   "source": [
    "## 6 - Créer une instance de notres CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1slU0EyXLuWr"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3,4,5]\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eC0EJAwLLuWu",
    "outputId": "36446cdb-3018-4363-aa0a-fc63098df2a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "        ...,\n",
       "        [ 0.4870, -0.3286,  0.6392,  ..., -0.4184, -0.0256,  0.1911],\n",
       "        [-0.3896, -0.0554,  0.4922,  ..., -0.0182, -0.8245,  0.0696],\n",
       "        [ 0.1829,  0.1536, -0.1446,  ..., -0.1389,  0.3579,  0.8286]])"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNDsXPRJLuW1"
   },
   "source": [
    "## 7 - Entrainer le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BVADtymyLuW3",
    "outputId": "de981dda-a113-4299-b64b-47937e1dbb89"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 398045/400000 [00:30<00:00, 26373.16it/s]"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NH8WT9CSLuW6"
   },
   "source": [
    "Implémenter la fonction qui calcule la précision\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "az-c9zfwLuW7"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y.float()).float() #convert into float for division \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18CGWz6XLuW-"
   },
   "source": [
    "Définir un fonction pour entrainer notre modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggAOZpM6LuW_"
   },
   "outputs": [],
   "source": [
    "def train_model(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.text).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label.float())\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STUa1L7PLuXC"
   },
   "source": [
    "Définir une fonction pour tester notre modèle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dXd9-JLsLuXE"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label.float())\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6-pbGKqhLuXI",
    "outputId": "2f85a0c9-fa3e-466b-cb41-883641fcd2ba",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 0.495, Train Acc: 75.19%, Val. Loss: 0.000, Val. Acc: 0.00%\n",
      "Epoch: 02, Train Loss: 0.302, Train Acc: 87.24%, Val. Loss: 0.000, Val. Acc: 0.00%\n",
      "Epoch: 03, Train Loss: 0.216, Train Acc: 91.37%, Val. Loss: 0.000, Val. Acc: 0.00%\n",
      "Epoch: 04, Train Loss: 0.144, Train Acc: 94.44%, Val. Loss: 0.000, Val. Acc: 0.00%\n",
      "Epoch: 05, Train Loss: 0.089, Train Acc: 97.01%, Val. Loss: 0.000, Val. Acc: 0.00%\n"
     ]
    }
   ],
   "source": [
    "  N_EPOCHS = 5\n",
    "  for epoch in range(N_EPOCHS):\n",
    "\n",
    "      train_loss, train_acc = train_model(model, train_iterator, optimizer, criterion)\n",
    "      #valid_loss, valid_acc = evaluate_model(model, valid_iterator, criterion)\n",
    "      valid_loss=0\n",
    "      valid_acc=0\n",
    "\n",
    "      print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc*100:.2f}%, Val. Loss: {valid_loss:.3f}, Val. Acc: {valid_acc*100:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNnkGoitLuXM"
   },
   "source": [
    "...and get our best test accuracy yet! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xmLlA56oLuXN",
    "outputId": "db38e2a8-7801-42a2-cc90-bae78424763b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.313, Test Acc: 88.07%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate_model(model, test_iterator, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pu9SMspYLuXS"
   },
   "source": [
    "## User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k1N8dcUkv-Mr"
   },
   "outputs": [],
   "source": [
    "# Some auxiliary functions in order to make a color pallete. \n",
    "# Couresy of https://www.oreilly.com/library/view/python-cookbook/0596001673/ch09s11.html\n",
    "\n",
    "import math\n",
    "\n",
    "def floatRgb(mag):\n",
    "    \"\"\" Return a tuple of floats between 0 and 1 for R, G, and B. \"\"\"\n",
    "    \n",
    "    #blue  = min((max((4*(0.75-x), 0.)), 1.))\n",
    "    #red   = min((max((4*(x-0.25), 0.)), 1.))\n",
    "    #green = min((max((4*math.fabs(x-0.5)-1., 0.)), 1.))\n",
    "    red=0\n",
    "    blue=0\n",
    "    green =0\n",
    "    \n",
    "    if mag>0:\n",
    "      blue=min(1,mag/3)\n",
    "    else:\n",
    "      if mag<0:\n",
    "        red=min(1,-mag/3)\n",
    "        \n",
    "        \n",
    "     \n",
    "    \n",
    "    return red, green, blue\n",
    "  \n",
    "def rgb(mag):\n",
    "    \"\"\" Return a tuple of integers, as used in AWT/Java plots. \"\"\"\n",
    "    red, green, blue = floatRgb(mag)\n",
    "    return int(red*255), int(green*255), int(blue*255)\n",
    "\n",
    "def strRgb(mag):\n",
    "    \"\"\" Return a hex string, as used in Tk plots. \"\"\"\n",
    "    return \"#%02x%02x%02x\" % rgb(mag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PbedQ2sZLuXS"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "from IPython.core.display import display,HTML\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "def predict_sentiment(sentence, explain_scores=True,explain_relative_to=1):\n",
    "  \n",
    "    tokenized_sentence = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed_sentence = [TEXT.vocab.stoi[t] for t in tokenized_sentence]\n",
    "    tensor = torch.LongTensor(indexed_sentence).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    \n",
    "    original_input_embedding,input_grad=get_input_gradients(indexed_sentence,prediction,explain_relative_to)\n",
    "    \n",
    "    explanation=get_prediction_explanation(tokenized_sentence,original_input_embedding,input_grad,explain_scores)\n",
    "      \n",
    "    return {'tokenized_sentence': tokenized_sentence,'prediction': prediction.item(),'explanation': explanation }\n",
    "\n",
    "def get_input_gradients(original_sentence,prediction,in_relation_to):\n",
    "    gradient_truth=torch.Tensor([in_relation_to]).unsqueeze(0)\n",
    "    if torch.cuda.is_available():\n",
    "      gradient_truth=gradient_truth.cuda()\n",
    "    \n",
    "    loss=criterion(prediction,gradient_truth)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    input_grad=torch.Tensor(len(original_sentence),model.embedding.weight.size(1))\n",
    "    original_input_embedding=torch.Tensor(len(original_sentence),model.embedding.weight.size(1))\n",
    "    \n",
    "    for i in range(0,len(original_sentence)):\n",
    "      original_input_embedding[i]=model.embedding.weight[original_sentence[i]]\n",
    "      input_grad[i]=model.embedding.weight.grad[original_sentence[i]]\n",
    "    \n",
    "    return original_input_embedding,input_grad\n",
    "    \n",
    "    \n",
    "\n",
    "def get_input_scores(input,input_embedding,input_grad):\n",
    "  \n",
    "  \n",
    "  # Take a SGD step using grads\n",
    "  \n",
    "  input_after_step=input_embedding-input_grad\n",
    "  after_grad_norms = torch.norm(input_after_step, 2, 1)\n",
    "  before_grad_norms = torch.norm(input_embedding, 2, 1)\n",
    "  variation = after_grad_norms-before_grad_norms\n",
    " \n",
    "  standard_deviation=torch.std(variation)\n",
    "  mean=torch.mean(variation)\n",
    "  z_score=(variation-mean)/standard_deviation\n",
    " \n",
    "  return z_score\n",
    "\n",
    "def old_get_input_scores(input,input_embedding,input_grad):\n",
    "  \n",
    "  grad_norms=torch.norm(input_grad,2,1)\n",
    "  \n",
    "  return grad_norms/torch.max(grad_norms)\n",
    "\n",
    "  \n",
    "def get_prediction_explanation(input,input_embedding,input_grad, explain_scores):\n",
    " \n",
    "  \n",
    "  input_word_scores=get_input_scores(input,input_embedding,input_grad)\n",
    "   \n",
    "  explanation=\"\"\n",
    "  for i in range(0,len(input)):\n",
    "    token=input[i]\n",
    "    token_color=strRgb(input_word_scores[i])\n",
    "    if explain_scores:\n",
    "      str_token=\"%s (%.3f)\"%(token,input_word_scores[i])\n",
    "    else:\n",
    "      str_token=token\n",
    "    \n",
    "    explanation=explanation+'<font color=\"'+token_color+'\">'+str_token+'&nbsp;</font>'\n",
    "    if i>0 and i%20==0:\n",
    "      explanation=explanation+\"<br/>\"\n",
    "    \n",
    "  return {'word_scores': input_word_scores,'input_gradient': input_grad,'textual_explanation':explanation }\n",
    "\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VueXNHCzHC9w"
   },
   "outputs": [],
   "source": [
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "def get_projected_words(word,word_gradient,num_words=1):\n",
    "  \n",
    "  word_index=TEXT.vocab.stoi[word]\n",
    "  word_embedding=TEXT.vocab.vectors[word_index]\n",
    "  learning_rate=1\n",
    "  i=0\n",
    "  result=[]\n",
    "  \n",
    "  while i<100000:\n",
    "    try: \n",
    "      word_embedding=word_embedding-learning_rate*word_gradient\n",
    "    except:\n",
    "      # We can have a float overflow here if this process gets out of control\n",
    "      return result\n",
    "    similarity_value,similarity_index=cosine_similarity(word_embedding.unsqueeze(0),TEXT.vocab.vectors,dim=1).sort(descending=True)\n",
    "    if similarity_index[0]!=word_index:\n",
    "      if  similarity_value[0]<0.5:\n",
    "        break\n",
    "      \n",
    "      result.append({'word':TEXT.vocab.itos[similarity_index[0]],'similarity': similarity_value[0]})\n",
    "      word_index=similarity_index[0]\n",
    "      learning_rate=1\n",
    "      if len(result)>=num_words:\n",
    "        break\n",
    "      \n",
    "    i=i+1\n",
    "    learning_rate=learning_rate*1.1\n",
    "      \n",
    "  return result\n",
    "\n",
    "  \n",
    "def get_projected_sentence_word(prediction,word):\n",
    "  \n",
    "  sentence=prediction['tokenized_sentence']\n",
    "  word_index_in_sentence=[i for i in range(0,len(sentence)) if sentence[i]==word][0]\n",
    "  word_gradient=prediction['explanation']['input_gradient'][word_index_in_sentence]\n",
    "  \n",
    "  \n",
    "  return get_projected_words(word,word_gradient,1)\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "jIm9_KJRLuXb",
    "outputId": "ceb55778-c406-450f-c544-1fe383dca09f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24004195630550385\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<font color=\"#000017\">This (0.282)&nbsp;</font><font color=\"#00003b\">is (0.704)&nbsp;</font><font color=\"#000036\">a (0.640)&nbsp;</font><font color=\"#fc0000\">ridiculous (-2.971)&nbsp;</font><font color=\"#270000\">movie (-0.467)&nbsp;</font><font color=\"#000045\">and (0.820)&nbsp;</font><font color=\"#00002a\">you (0.494)&nbsp;</font><font color=\"#00000e\">should (0.171)&nbsp;</font><font color=\"#000006\">never (0.081)&nbsp;</font><font color=\"#000006\">see (0.081)&nbsp;</font><font color=\"#000006\">it (0.081)&nbsp;</font><font color=\"#000006\">. (0.081)&nbsp;</font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'word': 'interesting', 'similarity': tensor(0.7075)}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "prediction=predict_sentiment(\"This is a ridiculous movie and you should never see it.\")\n",
    "print(prediction['prediction'])\n",
    "display(HTML(prediction['explanation']['textual_explanation']))\n",
    "print(get_projected_sentence_word(prediction,'ridiculous'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "sC7nv-Xe3Xne",
    "outputId": "35eea51d-42bd-424b-b4ca-cc7278d45c7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9992479681968689\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<font color=\"#00000e\">I (0.175)&nbsp;</font><font color=\"#00009f\">affection (1.873)&nbsp;</font><font color=\"#630000\">Miiasaki (-1.173)&nbsp;</font><font color=\"#00005c\">memorable (1.082)&nbsp;</font><font color=\"#060000\">< (-0.080)&nbsp;</font><font color=\"#630000\">PAD (-1.173)&nbsp;</font><font color=\"#000017\">> (0.275)&nbsp;</font><font color=\"#060000\">< (-0.080)&nbsp;</font><font color=\"#630000\">PAD (-1.173)&nbsp;</font><font color=\"#000017\">> (0.275)&nbsp;</font>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "prediction=predict_sentiment(\"I affection Miiasaki memorable <PAD> <PAD>\")\n",
    "print(prediction['prediction'])\n",
    "display(HTML(prediction['explanation']['textual_explanation']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_CF20HeKoJU2",
    "outputId": "a2f14aed-2ab3-4244-f827-c3a461a1da6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(get_projected_sentence_word(prediction,'affection'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgz8GY9nVJRt"
   },
   "source": [
    "## 8 - Importer les fonctions de nlp.py pour avoir le bon sentiment des phrases \n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86hiSK8iQdJp",
    "outputId": "3ea215c1-fdd2-450c-e440-eafd95a936c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Calculons le vecteur associé au texte\n",
    "def text2vec(wv, idf, text):\n",
    "    text_vector = np.zeros(300)\n",
    "    weights = 0\n",
    "    # Pour tous les tokens du texte\n",
    "    for word in text:\n",
    "        try:\n",
    "            # On extrait le vecteur d'un mot\n",
    "            vector = wv.get_vector(word)\n",
    "            norm = np.linalg.norm(vector)\n",
    "            # On le normalise\n",
    "            vector = vector / norm\n",
    "            # On récupère l'idf du mot (voir TP2)\n",
    "            weight = idf[word]\n",
    "            # On pondère le vecteur avant de le rajouter au vecteur représentant le texte\n",
    "            text_vector += weight*vector\n",
    "            weights += weight\n",
    "        except KeyError:\n",
    "            pass\n",
    "    # On renormalise le vecteur\n",
    "    if weights > 0:\n",
    "        text_vector /= weights\n",
    "    return text_vector\n",
    "\n",
    "\n",
    "# Pris de la correction du TP2\n",
    "def extract_tokens(text):\n",
    "    res = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        tmp_res = nltk.word_tokenize(sent)\n",
    "        for token in tmp_res:\n",
    "            res += re.split(\"[./]\", token)\n",
    "    return res\n",
    "\n",
    "def clean_tokens(tokens):\n",
    "    return [token.lower() for token in tokens if token not in string.punctuation]\n",
    "\n",
    "def text2tokens(text):\n",
    "    tokens = extract_tokens(text)\n",
    "    tokens = clean_tokens(tokens)\n",
    "    return tokens\n",
    "## Fin de la correction TP2 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71ddHKHCqAKi",
    "outputId": "388d8db9-7894-45d1-fbf0-1d4e6f628f6b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.23.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Chargement modèle NLP \n",
    "import joblib\n",
    "# fichier nlp.py fouillez le !\n",
    "\n",
    "# Chargement modèle MNIST\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "nlp_model = joblib.load('nlp_model.joblib')\n",
    "ml = nlp_model[\"ml\"]\n",
    "idf = nlp_model[\"idf\"]\n",
    "wv = nlp_model[\"wv\"]\n",
    "\n",
    "def compute_sentiment(text, wv, idf, ml, threshold=0.55):\n",
    "    # NLP : feature exctraction\n",
    "    tokens = text2tokens(text)\n",
    "    vector = text2vec(wv, idf, tokens)\n",
    "    # Compute prediction\n",
    "    prediction = ml.predict_proba(vector.reshape(1, -1))[0]\n",
    "    # Use positive class proba and threshold to estimate sentiment\n",
    "    sentiment = (prediction[1] > threshold)\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FIde0cH-VyFX"
   },
   "outputs": [],
   "source": [
    "def display_message(message):\n",
    "  display(HTML(message))\n",
    "  \n",
    "def predict_and_make_it_better(text,better_direction=1):\n",
    "  \n",
    "  version=0\n",
    "  word_to_change=None\n",
    "  better_word=None\n",
    "  \n",
    "  while int(compute_sentiment(text, wv, idf, ml, threshold=0.55) == True) != better_direction:\n",
    "    \n",
    "    prediction=predict_sentiment(text,explain_scores=False,explain_relative_to=better_direction)\n",
    "    #display_message(\"<H2> Version \"+str(version)+\"</H2>\")\n",
    "    #if word_to_change!=None:\n",
    "      #display_message(\"<H3>\"+word_to_change+\"->\"+better_word+\"</H3>\")\n",
    "      \n",
    "    #display_message(\"<H3> Sentiment: \"+str(prediction['prediction'])+\"+</H3>\")\n",
    "    #display(HTML(prediction['explanation']['textual_explanation']))\n",
    "    \n",
    "    # Get the word with the highest absolute score\n",
    "    word_to_change=None\n",
    "    better_word=None\n",
    "  \n",
    "    word_scores=prediction['explanation']['word_scores']\n",
    "    _,sorted_indices=torch.abs(word_scores).sort(descending=True)\n",
    "    changed_text=False\n",
    "    for i in range(0,sorted_indices.size(0)):\n",
    "      tokenized_sentence=prediction['tokenized_sentence']\n",
    "      word_to_change=tokenized_sentence[sorted_indices[i]]\n",
    "      better_words=get_projected_sentence_word(prediction,word_to_change)\n",
    "      if len(better_words)>0:\n",
    "        better_word=better_words[0]['word']\n",
    "        new_tokenized_sentence=[t if t!=word_to_change else better_word for t in tokenized_sentence]\n",
    "        text=\" \".join(new_tokenized_sentence)\n",
    "        changed_text=True\n",
    "        break\n",
    "    \n",
    "    if not changed_text:\n",
    "      return\n",
    "    \n",
    "    version=version+1\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CfCRGpjfYDFB",
    "outputId": "521b5b31-7d3d-4b60-d6d8-ef78341b55e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good i crazy not funny\n"
     ]
    }
   ],
   "source": [
    "print(predict_and_make_it_better(\"bad worse stupid not funny\",better_direction=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E7wGKBjvamrw"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# Lecture de \"l'email\"\n",
    "with open(\"new_email.json\", \"r\") as fp:\n",
    "    email = json.load(fp)\n",
    "\n",
    "\n",
    "\n",
    "def create_new_email(email):\n",
    "    new_email = email.copy()\n",
    "    count = 0\n",
    "    for student in new_email:\n",
    "        count += 1\n",
    "        if(count < 19):\n",
    "            student['appreciation'] = predict_and_make_it_better(student['appreciation'], 1)\n",
    "        else :\n",
    "            student['appreciation'] = predict_and_make_it_better(student['appreciation'], 0)\n",
    " \n",
    "\n",
    "    return new_email\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kh-aOIYOmA0Q"
   },
   "source": [
    "## 9 -  Enregistrer les changements des emails dans un fichier Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NaxSTPTvc4Yg",
    "outputId": "d0c85921-4d0b-4b5e-99eb-eca96d876a06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "friendship giving too much away , there is a fade to white an hour into the film . True\n",
      "My wife and I really had high hopes for this film , but it was a major expressed . True\n",
      "And how come the Whoop never changes her hair or glasses over the many years this film showcases ? True\n",
      "I seriously feel like this is something that a screenwriting student would be written in a Quentin Tarantino / Eddie Murphy phase and unique True\n",
      "Welles would go from there to explore the mystery narrative and the self-reference of Shakespeare with this eye. True\n",
      "The idea of getting to spirit was n't yet for her , unique is became dedication hook between True\n",
      "This is the wonderful of lauded - horror item you 'd find packaged in with 50 other random cheesefests and and row programmers . True\n",
      "met , even at the age of 7 and could tell that and was watching collection . True\n",
      "It 's rare G - rated showcase fare and at least you do n't have to worry about leaving your kids alone while they watch it . True\n",
      "I remember being shocked at the spirit provides of McNairy County and the brutality of the \" redneck gangs \" . True\n",
      "Even if it were unique funny , great mouldy waxwork of a film would still be soberingly disrespectful . True\n",
      "The good guys have to team up with the good guys in a town where they are able to escape from . True\n",
      "Pavignano and D'Alatri were very excellent at writing , and that is one of the strong points of the movie . True\n",
      "There 's a reason a writer also n't direct his work and even act throughout it also splendid addition everyone enjoyed n't do it all . True\n",
      "There is nothing in this movie that hasn't been done a thousand times in other gangster flicks and done much better. True\n",
      "Although I 'll take that back for now ..... it hinges greatly on her next film . young , and her sister is way stupid . True\n",
      "And it seems the Australian press is just as accomplished at misery - inspiring pursuit and overkill as their American colleagues . True\n",
      "whose they do n't show on cable or make it available on video , no great and Too good . True\n",
      "The theater was dead with silence 'cause everyone was embarrassed to be in there watching such trash. False\n",
      "This film might have weak production values, but that is also what makes it so good. False\n"
     ]
    }
   ],
   "source": [
    "new_email = create_new_email(email)\n",
    "for student in new_email :\n",
    "    sentiment = compute_sentiment(student['appreciation'], wv, idf, ml)\n",
    "    print(student['appreciation'], sentiment)\n",
    "\n",
    "with open(\"email_perf.json\", \"w\") as ne:\n",
    "    json.dump(new_email, ne)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Convolutional Sentiment Analysis.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
